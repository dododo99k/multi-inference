{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg', force=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib,json,os\n",
    "# matplotlib.use('Agg', force=True)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read results in ./profile_results/results/ and get average inference time\n",
    "\n",
    "model_name='resnet50'\n",
    "parallel=1\n",
    "ee_head=0\n",
    "results_files = os.listdir('./profile_results_resnet50/results/')\n",
    "\n",
    "batch_sizes = [0]\n",
    "inference_times = [0]\n",
    "for f in results_files:\n",
    "    name_parts = f.split('_')\n",
    "    if name_parts[0] == model_name and name_parts[2] == str(parallel):\n",
    "        pass\n",
    "    else:\n",
    "        continue\n",
    "    with open(f'./profile_results_resnet50/results/{f}', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        batch_sizes.append(data[\"meta\"][\"batch\"])\n",
    "        inference_times.append(data[\"summary\"][\"mean_ms\"])\n",
    "# sort by batch size\n",
    "batch_sizes, inference_times = zip(*sorted(zip(batch_sizes, inference_times)))\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(batch_sizes[:-1], inference_times[:-1], marker='o', label='inference time')\n",
    "\n",
    "\n",
    "# 只用前 10 个点做一次线性回归\n",
    "x = np.asarray(batch_sizes[1:11], dtype=float)\n",
    "y = np.asarray(inference_times[1:11], dtype=float)\n",
    "coef = np.polyfit(x, y, 1)          # coef[0] 是斜率，coef[1] 是截距\n",
    "fit_fn = np.poly1d(coef)\n",
    "\n",
    "x_line = np.linspace(min(batch_sizes), max(batch_sizes[:-2]), 400)\n",
    "plt.plot(x_line, fit_fn(x_line), color='r', linestyle='-', label='1st-order fit (first 10)')\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Average Inference Time (ms)')\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(f'Batch Size vs Average Inference Time for {model_name}, EE Head={ee_head}')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(f'{model_name}_inference_time_vs_batch_size.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found batch sizes: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n",
      "Found EE heads: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "import os,json\n",
    "import numpy as np\n",
    "import matplotlib,json,os\n",
    "# matplotlib.use('Agg', force=True)\n",
    "import matplotlib.pyplot as plt\n",
    "model_name='resnet50ee'\n",
    "parallel=1\n",
    "results_files = os.listdir('./profile_results/results/')\n",
    "\n",
    "batch_size_set = set()\n",
    "ee_head_set = set()\n",
    "for f in results_files:\n",
    "    name_parts = f[:-5].split('_')\n",
    "    if name_parts[0] == model_name and name_parts[2] == str(parallel):\n",
    "        pass\n",
    "    else:\n",
    "        continue\n",
    "    batch_size_set.add(int(name_parts[4]))\n",
    "    ee_head_set.add(int(name_parts[6]))\n",
    "print(f'Found batch sizes: {sorted(batch_size_set)}')\n",
    "print(f'Found EE heads: {sorted(ee_head_set)}')\n",
    "\n",
    "ee_head_nums = sorted(ee_head_set)[-1]\n",
    "batch_nums = sorted(batch_size_set)[-1]\n",
    "\n",
    "# inference_times_all = [[0 for _ in range(ee_head_nums + 1)] for _ in range(batch_nums + 1)]\n",
    "inference_times_all = np.zeros((batch_nums + 1, ee_head_nums + 1))\n",
    "for f in results_files:\n",
    "    name_parts = f[:-5].split('_')\n",
    "    if name_parts[0] == model_name and name_parts[2] == str(parallel):\n",
    "        pass\n",
    "    else:\n",
    "        continue\n",
    "    batch = int(name_parts[4])\n",
    "    ee_head = int(name_parts[6])\n",
    "    with open(f'./profile_results/results/{f}', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        inference_times_all[batch][ee_head] = data[\"summary\"][\"mean_ms\"]\n",
    "        # print(f'Batch {batch}, EE Head {ee_head}, Inference Time: {data[\"summary\"][\"mean_ms\"]} ms')\n",
    "with open(f'./profile_results/{model_name}_inference_times.json', \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(inference_times_all.tolist(), f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw all inference times by plot\n",
    "batch_list = list(range(batch_nums + 1))\n",
    "color_list = ['blue', 'green', 'red', 'cyan',\n",
    "'magenta', 'yellow', 'black', 'white', 'gray', 'grey', 'orange', 'purple', 'brown', 'pink']\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# inference_times_all\n",
    "for i in range(ee_head_nums+1):\n",
    "    ee_head_inference_times = inference_times_all[:,i]\n",
    "    plt.plot(batch_list, ee_head_inference_times, marker='o', markersize=2, color=color_list[i % len(color_list)], label=f'EE Head {i}')\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Average Inference Time (ms)')\n",
    "plt.legend()\n",
    "plt.title(f'Batch Size vs Average Inference Time for {model_name}, EE Head num={ee_head_nums}')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(f'{model_name}_inference_time_vs_batch_size.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw efficiency curves\n",
    "batch_list = list(range(batch_nums + 1))\n",
    "color_list = ['blue', 'green', 'red', 'cyan',\n",
    "'magenta', 'yellow', 'black', 'white', 'gray', 'grey', 'orange', 'purple', 'brown', 'pink']\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "inference_times_all\n",
    "for i in range(ee_head_nums+1):\n",
    "    first_head_time = inference_times_all[1,i] # ignore batch 0\n",
    "    ee_head_efficiency =  first_head_time* np.asarray(batch_list[1:])/inference_times_all[1:,i]\n",
    "    plt.plot(batch_list[1:], ee_head_efficiency, marker='o', markersize=2, color=color_list[i % len(color_list)], label=f'EE Head {i}')\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Inference Efficiency')\n",
    "plt.legend(    fontsize=8,       # 字体变小\n",
    "    markerscale=0.8,  # 图例中点/线的缩放\n",
    "    handlelength=1.0, # 图例线段长度变短\n",
    "    borderpad=0.3,    # 边框内边距变小\n",
    "    labelspacing=0.3, # 各行之间间距变小\n",
    "    )\n",
    "plt.title(f'Batch Size vs Inference Efficiency for {model_name}, EE Head num={ee_head_nums}')\n",
    "plt.grid(True)\n",
    "\n",
    "# plt.savefig(f'{model_name}_inference_time_vs_batch_size.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [ 1.114  1.05   0.976  0.85   0.782  0.745  0.679  0.606  0.558  0.475  0.418  0.364  0.322  0.234]\n",
      " [ 1.26   1.194  1.089  0.887  0.823  0.765  0.704  0.645  0.583  0.489  0.438  0.383  0.331  0.245]\n",
      " [ 1.516  1.448  1.34   1.145  1.069  0.99   0.921  0.843  0.773  0.655  0.585  0.513  0.441  0.324]\n",
      " [ 1.661  1.587  1.475  1.281  1.2    1.118  1.037  0.955  0.871  0.722  0.647  0.574  0.499  0.381]\n",
      " [ 1.904  1.817  1.693  1.486  1.393  1.308  1.215  1.125  1.039  0.865  0.78   0.69   0.605  0.467]\n",
      " [ 2.235  2.131  1.985  1.748  1.649  1.536  1.434  1.323  1.224  1.034  0.93   0.823  0.718  0.534]\n",
      " [ 2.469  2.344  2.204  1.919  1.843  1.705  1.629  1.492  1.385  1.225  1.107  0.993  0.855  0.658]\n",
      " [ 2.674  2.549  2.379  2.123  2.005  1.896  1.782  1.661  1.547  1.354  1.236  1.112  0.989  0.782]\n",
      " [ 2.987  2.878  2.69   2.37   2.232  2.114  1.974  1.851  1.715  1.478  1.344  1.215  1.073  0.858]\n",
      " [ 3.166  3.057  2.879  2.529  2.409  2.264  2.144  2.01   1.876  1.638  1.493  1.349  1.205  0.96 ]\n",
      " [ 3.767  3.649  3.463  3.098  2.925  2.766  2.596  2.425  2.264  1.946  1.781  1.596  1.43   1.11 ]\n",
      " [ 3.996  3.882  3.68   3.304  3.15   2.963  2.81   2.622  2.448  2.127  1.96   1.761  1.586  1.247]\n",
      " [ 4.266  4.139  3.93   3.54   3.378  3.185  3.026  2.83   2.653  2.324  2.144  1.935  1.744  1.385]\n",
      " [ 4.518  4.39   4.156  3.757  3.615  3.388  3.247  3.022  2.842  2.502  2.327  2.1    1.913  1.515]\n",
      " [ 4.705  4.574  4.45   3.98   3.794  3.682  3.419  3.23   3.114  2.692  2.535  2.278  2.089  1.673]\n",
      " [ 4.992  4.863  4.729  4.258  4.068  3.95   3.694  3.503  3.38   2.955  2.78   2.526  2.313  1.877]\n",
      " [ 5.724  5.594  5.318  4.753  4.547  4.377  4.116  3.905  3.738  3.323  3.108  2.85   2.609  2.133]\n",
      " [ 5.995  5.858  5.668  5.174  4.954  4.764  4.505  4.283  4.089  3.654  3.419  3.154  2.918  2.407]\n",
      " [ 6.358  6.227  6.005  5.523  5.298  5.083  4.831  4.601  4.392  3.957  3.709  3.437  3.19   2.658]\n",
      " [ 6.729  6.591  6.342  5.89   5.658  5.424  5.189  4.958  4.723  4.293  4.026  3.743  3.473  2.924]\n",
      " [ 7.821  7.621  7.307  6.715  6.448  6.178  5.91   5.642  5.372  4.839  4.498  4.15   3.805  3.159]\n",
      " [ 8.103  7.904  7.593  6.991  6.719  6.448  6.176  5.903  5.631  5.108  4.752  4.384  4.024  3.349]\n",
      " [ 8.59   8.383  8.051  7.449  7.149  6.846  6.544  6.242  5.939  5.376  5.004  4.624  4.244  3.532]\n",
      " [ 8.882  8.675  8.327  7.719  7.418  7.117  6.818  6.519  6.216  5.646  5.254  4.854  4.459  3.716]\n",
      " [ 9.409  9.193  8.828  8.14   7.823  7.505  7.191  6.875  6.558  5.965  5.548  5.131  4.717  3.912]\n",
      " [ 9.645  9.454  9.109  8.448  8.128  7.808  7.49   7.169  6.848  6.243  5.812  5.374  4.937  4.097]\n",
      " [10.034  9.847  9.499  8.836  8.506  8.178  7.857  7.528  7.196  6.591  6.127  5.661  5.194  4.316]\n",
      " [10.48  10.251  9.864  9.153  8.814  8.473  8.148  7.807  7.465  6.852  6.374  5.892  5.412  4.493]\n",
      " [10.81  10.584 10.201  9.478  9.129  8.777  8.444  8.093  7.743  7.131  6.624  6.124  5.626  4.67 ]\n",
      " [11.054 10.852 10.494  9.79   9.433  9.075  8.737  8.379  8.02   7.394  6.88   6.362  5.847  4.847]\n",
      " [11.385 11.179 10.818 10.106  9.742  9.378  9.032  8.668  8.305  7.665  7.133  6.595  6.061  5.03 ]\n",
      " [11.939 11.705 11.314 10.571 10.203  9.834  9.478  9.11   8.739  8.056  7.504  6.954  6.398  5.239]\n",
      " [12.571 12.335 11.905 11.122 10.736 10.349  9.971  9.584  9.197  8.446  7.836  7.232  6.627  5.404]\n",
      " [13.015 12.748 12.278 11.474 11.082 10.69  10.302  9.91   9.517  8.754  8.124  7.489  6.854  5.577]\n",
      " [13.276 13.052 12.638 11.857 11.454 11.051 10.65  10.246  9.843  9.072  8.406  7.744  7.073  5.748]\n",
      " [13.649 13.426 13.008 12.217 11.812 11.404 10.996 10.588 10.18   9.4    8.702  8.002  7.296  5.922]\n",
      " [14.099 13.854 13.415 12.608 12.196 11.783 11.371 10.958 10.544  9.753  9.02   8.284  7.548  6.131]\n",
      " [14.445 14.197 13.757 12.943 12.525 12.109 11.692 11.274 10.857 10.058  9.302  8.54   7.778  6.312]\n",
      " [14.766 14.529 14.093 13.279 12.858 12.436 12.014 11.591 11.167 10.358  9.576  8.788  8.004  6.481]\n",
      " [15.109 14.871 14.434 13.61  13.182 12.753 12.325 11.897 11.468 10.648  9.843  9.034  8.226  6.656]\n",
      " [15.592 15.361 14.913 14.049 13.602 13.155 12.705 12.256 11.806 10.962 10.132  9.306  8.475  6.828]\n",
      " [17.052 16.694 16.114 15.055 14.542 14.031 13.521 13.009 12.496 11.495 10.589  9.68   8.773  7.028]\n",
      " [17.867 17.526 16.747 15.377 14.858 14.337 13.82  13.301 12.78  11.762 10.837  9.914  8.989  7.201]\n",
      " [18.225 17.895 17.115 15.715 15.186 14.646 14.118 13.589 13.062 12.029 11.085 10.14   9.197  7.379]\n",
      " [18.066 17.7   17.106 16.026 15.488 14.951 14.413 13.877 13.336 12.286 11.325 10.363  9.402  7.552]\n",
      " [18.421 18.051 17.453 16.369 15.819 15.272 14.722 14.173 13.624 12.549 11.572 10.592  9.611  7.727]\n",
      " [19.354 18.988 18.186 16.737 16.18  15.623 15.063 14.503 13.943 12.847 11.851 10.855  9.857  7.915]\n",
      " [20.01  19.659 18.821 17.354 16.734 16.118 15.501 14.88  14.263 13.143 12.126 11.107 10.096  8.122]\n",
      " [19.833 19.463 18.835 17.707 17.066 16.427 15.787 15.148 14.505 13.331 12.271 11.211 10.153  8.317]\n",
      " [20.226 19.844 19.202 18.064 17.409 16.753 16.101 15.446 14.79  13.599 12.524 11.448 10.369  8.5  ]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(linewidth=120)\n",
    "print(np.round(inference_times_all, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.91016545, 2.16520289, 2.45434607, 2.50527059,\n",
       "       2.62542279, 2.48766911, 2.3908166 , 2.45342242, 2.43403989,\n",
       "       2.31582123, 2.2495062 , 2.1938206 , 2.16075045, 2.09603819,\n",
       "       1.9922971 , 1.86327524, 1.74835722, 1.67075459, 1.59913131,\n",
       "       1.55385392, 1.53557726, 1.52213541, 1.50986872, 1.49394245,\n",
       "       1.48338953, 1.46251127, 1.45671447, 1.45170971, 1.4468905 ,\n",
       "       1.44058255, 1.42794951, 1.42749091, 1.42505489, 1.42333472,\n",
       "       1.42095075, 1.41065653, 1.40723152, 1.40658709, 1.40482136,\n",
       "       1.40365284, 1.39697108, 1.39581713, 1.39380713, 1.39287946,\n",
       "       1.39167291, 1.38808468, 1.38153731, 1.37717605, 1.37513634])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_head_time* np.asarray(batch_list[1:])/inference_times_all[1:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib,json,os\n",
    "# matplotlib.use('Agg', force=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def plot_poisson(lam, k_max=None):\n",
    "    if k_max is None:\n",
    "        k_max = int(lam + 5 * lam**0.5)\n",
    "    ks = np.arange(0, k_max + 1)\n",
    "    pmf = np.exp(-lam) * np.power(lam, ks) / np.array([math.factorial(k) for k in ks])\n",
    "\n",
    "    plt.plot(ks, pmf, \"o\", color=\"tab:blue\", label=\"PMF\")   # 点\n",
    "    plt.vlines(ks, 0, pmf, colors=\"tab:blue\", alpha=0.2)    # 细线可选\n",
    "    plt.title(f\"Poisson PMF (lambda={lam})\")\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"P(K=k)\")\n",
    "    plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_total_poisson(lam, n_slots, delta_t=1.0):\n",
    "    lam_tot = lam * delta_t * n_slots\n",
    "    k_max = int(lam_tot + 5 * math.sqrt(lam_tot))  # 覆盖右尾\n",
    "    ks = np.arange(0, k_max + 1)\n",
    "    pmf = np.exp(-lam_tot) * lam_tot**ks / np.array([math.factorial(k) for k in ks])\n",
    "    \n",
    "    plt.bar(ks, pmf, color=\"skyblue\", edgecolor=\"k\")\n",
    "    plt.title(f\"Total count in {n_slots} slots, Poisson(lam={lam_tot:.2f})\")\n",
    "    plt.xlabel(\"total event number K\")\n",
    "    plt.ylabel(\"P(K=k)\")\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "intensity = 4\n",
    "plot_poisson(intensity)\n",
    "plot_total_poisson(intensity, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 slots, estimated batch size: 2.00 estimated batch inference time:  1.1729570709850745\n",
      "2 slots, estimated batch size: 4.00 estimated batch inference time:  1.7113301715997051\n",
      "3 slots, estimated batch size: 6.00 estimated batch inference time:  2.2166835792223245\n",
      "4 slots, estimated batch size: 8.00 estimated batch inference time:  2.7594058536320043\n",
      "5 slots, estimated batch size: 10.00 estimated batch inference time:  3.3422716590663746\n",
      "6 slots, estimated batch size: 12.00 estimated batch inference time:  3.9630204102747255\n",
      "7 slots, estimated batch size: 14.00 estimated batch inference time:  4.634970460240015\n",
      "8 slots, estimated batch size: 16.00 estimated batch inference time:  5.376597818625582\n",
      "9 slots, estimated batch size: 18.00 estimated batch inference time:  6.187125606089939\n",
      "10 slots, estimated batch size: 20.00 estimated batch inference time:  7.042459279391524\n",
      "11 slots, estimated batch size: 22.00 estimated batch inference time:  7.909827496858522\n",
      "12 slots, estimated batch size: 24.00 estimated batch inference time:  8.76540270327345\n",
      "For 13 slots, left distribution sum beyond 50 is 0.000014\n",
      "13 slots, estimated batch size: 26.00 estimated batch inference time:  9.601606846803527\n",
      "For 14 slots, left distribution sum beyond 50 is 0.000108\n",
      "14 slots, estimated batch size: 28.00 estimated batch inference time:  10.423365130329506\n",
      "For 15 slots, left distribution sum beyond 50 is 0.000515\n",
      "15 slots, estimated batch size: 30.00 estimated batch inference time:  11.238535075231994\n",
      "For 16 slots, left distribution sum beyond 50 is 0.001931\n",
      "16 slots, estimated batch size: 32.00 estimated batch inference time:  12.045802058583842\n",
      "For 17 slots, left distribution sum beyond 50 is 0.005967\n",
      "17 slots, estimated batch size: 34.00 estimated batch inference time:  12.821292285065642\n",
      "For 18 slots, left distribution sum beyond 50 is 0.015618\n",
      "18 slots, estimated batch size: 36.00 estimated batch inference time:  13.505955498102567\n",
      "For 19 slots, left distribution sum beyond 50 is 0.035336\n",
      "19 slots, estimated batch size: 38.00 estimated batch inference time:  14.001354914403723\n",
      "For 20 slots, left distribution sum beyond 50 is 0.070332\n",
      "20 slots, estimated batch size: 40.00 estimated batch inference time:  14.183232850907137\n",
      "Warning: For 21 slots left distribution sum beyond 50 is 0.125021 significant probability mass beyond 50 events!\n",
      "Final answer:  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "def get_total_poisson(lam, n_slots, delta_t=1.0):\n",
    "    lam_tot = lam * delta_t * n_slots\n",
    "    k_max = int(lam_tot + 5 * math.sqrt(lam_tot))  # 覆盖右尾\n",
    "    ks = np.arange(0, k_max + 1)\n",
    "    pmf = np.exp(-lam_tot) * lam_tot**ks / np.array([math.factorial(k) for k in ks])\n",
    "    return pmf\n",
    "    # plt.bar(ks, pmf, color=\"skyblue\", edgecolor=\"k\")\n",
    "    # plt.title(f\"Total count in {n_slots} slots, Poisson(lam={lam_tot:.2f})\")\n",
    "    # plt.xlabel(\"total event number K\")\n",
    "    # plt.ylabel(\"P(K=k)\")\n",
    "    # plt.grid(axis=\"y\", alpha=0.3)\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "intensity = 2\n",
    "answer = []\n",
    "for slot in range(1,91):\n",
    "    n_slot_distribution = get_total_poisson(intensity, slot)\n",
    "    if len(n_slot_distribution) > 50:\n",
    "        left_distribution_sum = np.sum(n_slot_distribution[50:])\n",
    "        if left_distribution_sum > 0.1:\n",
    "            print(f'Warning: For {slot} slots left distribution sum beyond 50 is {left_distribution_sum:.6f} significant probability mass beyond 50 events!')\n",
    "            break\n",
    "        else:\n",
    "            print(f'For {slot} slots, left distribution sum beyond 50 is {left_distribution_sum:.6f}')\n",
    "        n_slot_distribution = n_slot_distribution[:50]\n",
    "    n_slot_est_batch_inference_time = 0.0\n",
    "    for i, prob in enumerate(n_slot_distribution):\n",
    "        n_slot_est_batch_inference_time += prob * inference_times_all[i][0]\n",
    "    if slot > n_slot_est_batch_inference_time:\n",
    "        answer.append(slot)\n",
    "    print(f'{slot} slots, estimated batch size: {intensity*slot:.2f} estimated batch inference time: ', n_slot_est_batch_inference_time)\n",
    "if answer:\n",
    "    print('Final answer: ', answer)\n",
    "else:\n",
    "    print('No valid slot number found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate lambda from generated traffic: 1.017\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_traffic(intensity, duration):\n",
    "    # note: here intensity is in ms, and duration is also ms unit\n",
    "    # first step: generate arrival time first\n",
    "    arrival_times, t = [], 0\n",
    "    while t < duration:\n",
    "        # Generate the time until the next arrival using an exponential distribution\n",
    "        inter_arrival_time = np.random.exponential(1 / intensity)\n",
    "        t += inter_arrival_time\n",
    "        if t < duration:\n",
    "            arrival_times.append(t)\n",
    "\n",
    "    # second step: construct traffic in the time domain \n",
    "    traffic = np.zeros(int(duration))\n",
    "    indexs = np.asarray(arrival_times, dtype=int) # indexs could have multiple at the same position\n",
    "    uniques, counts = np.unique(indexs, return_counts=True)\n",
    "    traffic[uniques] = counts # assign counts to the position, not 1\n",
    "    return np.array(traffic, dtype=int)\n",
    "\n",
    "traffic = generate_traffic(intensity=1, duration=30000)\n",
    "\n",
    "print(f'estimate lambda from generated traffic: {traffic[-1000:].mean()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999.5\n"
     ]
    }
   ],
   "source": [
    "# get mean of the last 1000 ms traffic\n",
    "mean_last_1000 = np.mean(list(range(10000)))\n",
    "print(mean_last_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inference_times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
